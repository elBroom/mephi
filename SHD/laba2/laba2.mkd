Настройка на клиенте X11 (Linux)
```
# vim /etc/ssh/ssh_config
ForwardX11 yes
```
запуск `ssh -XC b14v7408@samos.dozen.mephi.ru`  
Source: http://wiki.enchtex.info/howto/ssh-forwardx11
# Лабораторная работа № 2
###Теория
**RAID 0** — дисковый массив из двух или более жёстких дисков без резервирования. Информация разбивается на блоки данных фиксированной длины и записывается на оба/несколько дисков поочередно, то есть один блок на первый диск, а второй блок на второй диск соответственно.  
***+***: Скорость считывания файлов увеличивается в n раз, где n — количество дисков. При этом такая оптимальная производительность достигается только для больших запросов, когда фрагменты файла находятся на каждом из дисков.  
***-***: Увеличивается вероятность потери данных: если вероятность отказа 1 диска равна p, то вероятность выхода из строя массива RAID 0 из двух дисков равна 2p+p*p. Таким образом, если вероятность отказа одного диска за год равна 1 %, то вероятность отказа массива RAID0 из двух дисков составляет 2,01 %, то есть практически в два раза больше.

**Строение RAID 0**:  
![raid0](RAID_0.png)  


  

**RAID 1** — массив из двух (или более) дисков, являющихся полными копиями друг друга. Не следует путать с массивами RAID 1+0, RAID 0+1 и RAID 10, в которых используются более сложные механизмы зеркалирования.  
***+***: Обеспечивает приемлемую скорость записи (такую же, как и без дублирования) и выигрыш по скорости чтения при распараллеливании запросов.  
***+***: Имеет высокую надёжность — работает до тех пор, пока функционирует хотя бы один диск в массиве. Вероятность выхода из строя сразу двух дисков равна произведению вероятностей отказа каждого диска, то есть значительно ниже вероятности выхода из строя отдельного диска. На практике при выходе из строя одного из дисков следует срочно принимать меры — вновь восстанавливать избыточность. Для этого с любым уровнем RAID (кроме нулевого) рекомендуют использовать диски горячего резерва.  
***-***: Недостаток RAID 1 в том, что по цене двух жестких дисков пользователь фактически получает объём лишь одного.  
Горячий диск подмены нужен для замены вышедшего из строя диска без остановки системы.

**Строение RAID 1**:  
![raid1](RAID_1.png) 

 У утилиты `mdadm` есть несколько режимов работы:  
`--create` — Создание нового массива из неиспользуемых устройств  
`--assemble` — Сборка ранее созданного массива  
`--build` — Создание или сборка массива без метаданных  
`--manage` — Изменить существующий архив  
`--grow` — Изменить размер активного массива  
`--incremental` — Добавить устройство в массив или удалить устройство из массива  
`--monitor` — Мониторить один или более массивов на предмет изменений  
`mdadm <устройство>` — Короткий вариант для mdadm --manage  

###Практика
1. Образы дисков:  
`ls disks/`  
![task1][ptask1]  
2. Запускаем в терминале:  
`sanvm`
3. Диски в системе:  
`fdisk -l`  
![task3][ptask3]  
4. Установленные пакет в системе:  
![task4][ptask4]  
5. Создаем RAID0 с чередованием на 3 диска с шагом 256KB  
`mdadm --create /dev/md0 --level=raid0 --chunk=256 --raid-devices=3 /dev/vda5 /dev/vdb5 /dev/vdc5`  
![task5][ptask5]  
6. Создаем RAID1 
`mdadm --create /dev/md1 --metadata=0.90 --level=raid1 --raid-devices=3 /dev/vda6 /dev/vdb6 /dev/vdc6`  
![task6][ptask6] 
7. Проверяем состояние дисков  
`cat /proc/mdstat`  
![task71][ptask71]  
`mdadm --detail /dev/md0`    
![task72][ptask72]  
`mdadm --detail /dev/md1` 
![task73][ptask73]  
8. Проверяем наличие конфигурационного файла  
![task8][ptask8]  
9. Создаем конфигурационный файл  
`echo "DEVICE partitions" > /etc/mdadm/mdadm.conf; mdadm --detail --scan --verbose >> /etc/mdadm/mdadm.conf` 
10. Создаем файловые системы  
`mkfs.xfs /dev/md0; mkfs.xfs /dev/md1`
11. Монтируем диски:
![task11][ptask11] 
12. Заполняем диски данными:
![task12][ptask12] 
13. Состояние raid1  
![task12][ptask13] 
14. `mdadm --manage /dev/md1 --fail /dev/vda6`  
![task14][ptask14]
15. Проверяем целостность данных  
![task15][ptask15]  
16. Удаляем сбойный диск  
`mdadm --manage /dev/md1 --remove /dev/vda6`  
![task16][ptask16]  
17. Удаляем еще один диск  
`mdadm --manage /dev/md1 --fail /dev/vdc6`  
`mdadm --manage /dev/md1 --remove /dev/vdc6`  
![task17][ptask17]  
18.  Проверяем данные raid1  
![task18][ptask18]  
19. Добавляем диск иммитируя замену
`mdadm --manage /dev/md1 --add /dev/vdc6 ; watch -n 1 "cat /proc/mdstat"`  
20. Добавляем диск горячей замены  
`mdadm --manage /dev/md1 --add /dev/vda6 ; watch -n 1 "cat /proc/mdstat"` 
21. Состояние raid1  
![task21][ptask21] 

Source: http://xgu.ru/wiki/mdadm

[ptask1]: lab2-1.png
[ptask3]: lab2-3.png
[ptask4]: lab2-4.png
[ptask5]: lab2-5.png
[ptask6]: lab2-6.png
[ptask71]: lab2-7-1.png
[ptask72]: lab2-7-2.png
[ptask73]: lab2-7-3.png
[ptask8]: lab2-8.png
[ptask11]: lab2-11.png
[ptask12]: lab2-12.png
[ptask13]: lab2-13.png
[ptask14]: lab2-14.png
[ptask15]: lab2-15.png
[ptask16]: lab2-16.png
[ptask17]: lab2-17.png
[ptask18]: lab2-18.png
[ptask21]: lab2-21.png